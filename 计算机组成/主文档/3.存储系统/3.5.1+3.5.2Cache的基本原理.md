# Cache 的工作原理与性能优化

## 摘要

通过分块机制与局部性原理，Cache 在 CPU 与主存间建立高速缓存层。采用命中率优化策略（顺序/同时访问）和写一致性控制，平均访问时间可缩短至 t_c + (1-h)t_m。后续将展开映射方式与替换算法研究。

---

## 主题

**分层缓存架构** | **时空局部性利用** | **块传输优化**  
通过 SRAM 高速介质实现 CPU 近端数据缓存，采用块传输机制降低访存延迟，运用命中率公式 h=t_c/(t_c+t_m)量化性能增益

> 重点难点
>
> - 块大小与命中率的非线性关系
> - 同时访问策略的时序冲突风险
> - 写直达/写回策略的能耗差异

---

## 线索区

### 1. Cache 架构原理

**定义**：CPU 与主存间的 SRAM 高速缓存层  
**结构特性**：

- 访问速度比主存快**5-10 倍**
- 采用哈佛结构（指令/数据分离）或统一结构  
  **物理实现**：
- 典型容量：L1 Cache **32-64KB**
- 工艺：6T SRAM 单元（面积比 DRAM 大**6-8 倍**）

### 2. 局部性原理

$$
\text{程序访存概率} =
\begin{cases}
k \cdot r^{-n} & \text{时间局部性（循环访问）} \\
k \cdot e^{-\lambda d} & \text{空间局部性（邻近访问）}
\end{cases}
$$

**典型应用**：

- 循环展开优化（增强时间局部性）
- 数组行优先存储（提升空间局部性）

### 3. 分块传输机制

**块结构**：

```txt
主存地址 = [块号: 块内地址]
Cache地址 = [标记: 块号: 块内地址]
```

**参数优化**：

- 最佳块大小经验公式：$B_{opt} = \sqrt{2T_m S}$  
  （T_m 为主存访问周期，S 为 Cache 容量）

### 4. 访问策略对比

| 策略     | 平均访问时间     | 硬件复杂度 | 适用场景         |
| -------- | ---------------- | ---------- | ---------------- |
| 顺序访问 | h·t_c + (1-h)t_m | 低         | 低功耗嵌入式系统 |
| 同时访问 | t_c + (1-h)t_m   | 高         | 高性能处理器     |

### 5. 写策略实现

**写直达(Write-through)**：

- 同步更新 Cache 和主存
- 数据一致性高，但增加**30%-40%**总线流量

**写回(Write-back)**：

- 仅更新 Cache，替换时写回主存
- 采用脏位(dirty bit)标记修改状态

---

## 总结区

**核心公式**：  
$t_{avg} = h \cdot t_c + (1-h) \cdot t_m$  
（h>98%时系统性能接近 Cache 速度）

**高频考点**：

1. 块大小对命中率的影响曲线
2. 同时访问策略的时序约束条件
3. 写策略选择与系统可靠性的权衡

**实验验证**：

- 使用 SimpleScalar 模拟器测试不同替换算法
- 通过 Valgrind 分析程序局部性特征

**延伸阅读**：

- 《计算机体系结构：量化研究方法》第 5 章
- IEEE TC 论文《Adaptive Cache Line Size Management》

需要调整内容细节或补充特定案例，请随时告知。
